{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIL95XzEIK4r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbUBH0aVIqPt"
   },
   "outputs": [],
   "source": [
    "# Normalize training set together with augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[x / 255.0 for x in[0.507, 0.487, 0.441]],\n",
    "                                     std=[x / 255.0 for x in [0.267, 0.256, 0.276]])\n",
    "])\n",
    "\n",
    "# Normalize test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[x / 255.0 for x in[0.507, 0.487, 0.441]],\n",
    "                                     std=[x / 255.0 for x in [0.267, 0.256, 0.276]])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "8bffefb88a124d4a9c0c209b1bde1950",
      "74ff845e25604f548f47fea8b629a4aa",
      "17ab7edf3843410182a0e1b1d2f992fe",
      "a923df6caf654c10adc55ff9c519bb2d",
      "3dd48e39941c4b83a6bb5181fccf9554",
      "8b89d0abb7194008bbce622ed5687265",
      "a15ab6aea3ca43809d12366dd3563b2c",
      "fc5d9dfdd01546b281c589d9b3e93959"
     ]
    },
    "colab_type": "code",
    "id": "4b3Niu-XIqOR",
    "outputId": "c27db1d0-15d2-449f-cceb-75aa145dc6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.001\n",
    "num_epochs = 40\n",
    "momentum=0.9\n",
    "weight_decay=1e-5\n",
    "batch_size_train=256\n",
    "batch_size_test=256\n",
    "batch_size = 256\n",
    "trainset = torchvision.datasets.CIFAR100(root = \"./data\",\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size_train, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root = \"./data\",\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=batch_size_test, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hgr934McIqKU",
    "outputId": "40589474-35b0-49ef-c5bd-ebde65f5c97e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \",device)\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0S09JjOIqIb"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "    \"\"\"\n",
    "\n",
    "    #BasicBlock and BottleNeck block\n",
    "    #have different output size\n",
    "    #we use class attribute expansion\n",
    "    #to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.LeakyReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 2\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.LeakyReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True))\n",
    "        #we use a different inputsize than the original paper\n",
    "        #so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "\n",
    "        # we have num_block blocks per layer, the first block\n",
    "        # could be 1 or 2, other blocks would always be 1\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = self.dropout(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def resnet34():\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "def resnet101():\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1-IfSV0BIqF0"
   },
   "outputs": [],
   "source": [
    "model = resnet50()\n",
    "model = model.cuda()# I choose ResNet50. Because of Memory :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Lj6lNHtYsK3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EdCp0VouX-qA",
    "outputId": "c291b1d3-3c03-4a35-c2c1-1f7135c0a709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "conv1.0.weight \t 1728\n",
      "conv1.1.weight \t 64\n",
      "conv1.1.bias \t 64\n",
      "conv2_x.0.residual_function.0.weight \t 4096\n",
      "conv2_x.0.residual_function.1.weight \t 64\n",
      "conv2_x.0.residual_function.1.bias \t 64\n",
      "conv2_x.0.residual_function.3.weight \t 36864\n",
      "conv2_x.0.residual_function.4.weight \t 64\n",
      "conv2_x.0.residual_function.4.bias \t 64\n",
      "conv2_x.0.residual_function.6.weight \t 8192\n",
      "conv2_x.0.residual_function.7.weight \t 128\n",
      "conv2_x.0.residual_function.7.bias \t 128\n",
      "conv2_x.0.shortcut.0.weight \t 8192\n",
      "conv2_x.0.shortcut.1.weight \t 128\n",
      "conv2_x.0.shortcut.1.bias \t 128\n",
      "conv2_x.1.residual_function.0.weight \t 8192\n",
      "conv2_x.1.residual_function.1.weight \t 64\n",
      "conv2_x.1.residual_function.1.bias \t 64\n",
      "conv2_x.1.residual_function.3.weight \t 36864\n",
      "conv2_x.1.residual_function.4.weight \t 64\n",
      "conv2_x.1.residual_function.4.bias \t 64\n",
      "conv2_x.1.residual_function.6.weight \t 8192\n",
      "conv2_x.1.residual_function.7.weight \t 128\n",
      "conv2_x.1.residual_function.7.bias \t 128\n",
      "conv2_x.2.residual_function.0.weight \t 8192\n",
      "conv2_x.2.residual_function.1.weight \t 64\n",
      "conv2_x.2.residual_function.1.bias \t 64\n",
      "conv2_x.2.residual_function.3.weight \t 36864\n",
      "conv2_x.2.residual_function.4.weight \t 64\n",
      "conv2_x.2.residual_function.4.bias \t 64\n",
      "conv2_x.2.residual_function.6.weight \t 8192\n",
      "conv2_x.2.residual_function.7.weight \t 128\n",
      "conv2_x.2.residual_function.7.bias \t 128\n",
      "conv3_x.0.residual_function.0.weight \t 16384\n",
      "conv3_x.0.residual_function.1.weight \t 128\n",
      "conv3_x.0.residual_function.1.bias \t 128\n",
      "conv3_x.0.residual_function.3.weight \t 147456\n",
      "conv3_x.0.residual_function.4.weight \t 128\n",
      "conv3_x.0.residual_function.4.bias \t 128\n",
      "conv3_x.0.residual_function.6.weight \t 32768\n",
      "conv3_x.0.residual_function.7.weight \t 256\n",
      "conv3_x.0.residual_function.7.bias \t 256\n",
      "conv3_x.0.shortcut.0.weight \t 32768\n",
      "conv3_x.0.shortcut.1.weight \t 256\n",
      "conv3_x.0.shortcut.1.bias \t 256\n",
      "conv3_x.1.residual_function.0.weight \t 32768\n",
      "conv3_x.1.residual_function.1.weight \t 128\n",
      "conv3_x.1.residual_function.1.bias \t 128\n",
      "conv3_x.1.residual_function.3.weight \t 147456\n",
      "conv3_x.1.residual_function.4.weight \t 128\n",
      "conv3_x.1.residual_function.4.bias \t 128\n",
      "conv3_x.1.residual_function.6.weight \t 32768\n",
      "conv3_x.1.residual_function.7.weight \t 256\n",
      "conv3_x.1.residual_function.7.bias \t 256\n",
      "conv3_x.2.residual_function.0.weight \t 32768\n",
      "conv3_x.2.residual_function.1.weight \t 128\n",
      "conv3_x.2.residual_function.1.bias \t 128\n",
      "conv3_x.2.residual_function.3.weight \t 147456\n",
      "conv3_x.2.residual_function.4.weight \t 128\n",
      "conv3_x.2.residual_function.4.bias \t 128\n",
      "conv3_x.2.residual_function.6.weight \t 32768\n",
      "conv3_x.2.residual_function.7.weight \t 256\n",
      "conv3_x.2.residual_function.7.bias \t 256\n",
      "conv3_x.3.residual_function.0.weight \t 32768\n",
      "conv3_x.3.residual_function.1.weight \t 128\n",
      "conv3_x.3.residual_function.1.bias \t 128\n",
      "conv3_x.3.residual_function.3.weight \t 147456\n",
      "conv3_x.3.residual_function.4.weight \t 128\n",
      "conv3_x.3.residual_function.4.bias \t 128\n",
      "conv3_x.3.residual_function.6.weight \t 32768\n",
      "conv3_x.3.residual_function.7.weight \t 256\n",
      "conv3_x.3.residual_function.7.bias \t 256\n",
      "conv4_x.0.residual_function.0.weight \t 65536\n",
      "conv4_x.0.residual_function.1.weight \t 256\n",
      "conv4_x.0.residual_function.1.bias \t 256\n",
      "conv4_x.0.residual_function.3.weight \t 589824\n",
      "conv4_x.0.residual_function.4.weight \t 256\n",
      "conv4_x.0.residual_function.4.bias \t 256\n",
      "conv4_x.0.residual_function.6.weight \t 131072\n",
      "conv4_x.0.residual_function.7.weight \t 512\n",
      "conv4_x.0.residual_function.7.bias \t 512\n",
      "conv4_x.0.shortcut.0.weight \t 131072\n",
      "conv4_x.0.shortcut.1.weight \t 512\n",
      "conv4_x.0.shortcut.1.bias \t 512\n",
      "conv4_x.1.residual_function.0.weight \t 131072\n",
      "conv4_x.1.residual_function.1.weight \t 256\n",
      "conv4_x.1.residual_function.1.bias \t 256\n",
      "conv4_x.1.residual_function.3.weight \t 589824\n",
      "conv4_x.1.residual_function.4.weight \t 256\n",
      "conv4_x.1.residual_function.4.bias \t 256\n",
      "conv4_x.1.residual_function.6.weight \t 131072\n",
      "conv4_x.1.residual_function.7.weight \t 512\n",
      "conv4_x.1.residual_function.7.bias \t 512\n",
      "conv4_x.2.residual_function.0.weight \t 131072\n",
      "conv4_x.2.residual_function.1.weight \t 256\n",
      "conv4_x.2.residual_function.1.bias \t 256\n",
      "conv4_x.2.residual_function.3.weight \t 589824\n",
      "conv4_x.2.residual_function.4.weight \t 256\n",
      "conv4_x.2.residual_function.4.bias \t 256\n",
      "conv4_x.2.residual_function.6.weight \t 131072\n",
      "conv4_x.2.residual_function.7.weight \t 512\n",
      "conv4_x.2.residual_function.7.bias \t 512\n",
      "conv4_x.3.residual_function.0.weight \t 131072\n",
      "conv4_x.3.residual_function.1.weight \t 256\n",
      "conv4_x.3.residual_function.1.bias \t 256\n",
      "conv4_x.3.residual_function.3.weight \t 589824\n",
      "conv4_x.3.residual_function.4.weight \t 256\n",
      "conv4_x.3.residual_function.4.bias \t 256\n",
      "conv4_x.3.residual_function.6.weight \t 131072\n",
      "conv4_x.3.residual_function.7.weight \t 512\n",
      "conv4_x.3.residual_function.7.bias \t 512\n",
      "conv4_x.4.residual_function.0.weight \t 131072\n",
      "conv4_x.4.residual_function.1.weight \t 256\n",
      "conv4_x.4.residual_function.1.bias \t 256\n",
      "conv4_x.4.residual_function.3.weight \t 589824\n",
      "conv4_x.4.residual_function.4.weight \t 256\n",
      "conv4_x.4.residual_function.4.bias \t 256\n",
      "conv4_x.4.residual_function.6.weight \t 131072\n",
      "conv4_x.4.residual_function.7.weight \t 512\n",
      "conv4_x.4.residual_function.7.bias \t 512\n",
      "conv4_x.5.residual_function.0.weight \t 131072\n",
      "conv4_x.5.residual_function.1.weight \t 256\n",
      "conv4_x.5.residual_function.1.bias \t 256\n",
      "conv4_x.5.residual_function.3.weight \t 589824\n",
      "conv4_x.5.residual_function.4.weight \t 256\n",
      "conv4_x.5.residual_function.4.bias \t 256\n",
      "conv4_x.5.residual_function.6.weight \t 131072\n",
      "conv4_x.5.residual_function.7.weight \t 512\n",
      "conv4_x.5.residual_function.7.bias \t 512\n",
      "conv5_x.0.residual_function.0.weight \t 262144\n",
      "conv5_x.0.residual_function.1.weight \t 512\n",
      "conv5_x.0.residual_function.1.bias \t 512\n",
      "conv5_x.0.residual_function.3.weight \t 2359296\n",
      "conv5_x.0.residual_function.4.weight \t 512\n",
      "conv5_x.0.residual_function.4.bias \t 512\n",
      "conv5_x.0.residual_function.6.weight \t 524288\n",
      "conv5_x.0.residual_function.7.weight \t 1024\n",
      "conv5_x.0.residual_function.7.bias \t 1024\n",
      "conv5_x.0.shortcut.0.weight \t 524288\n",
      "conv5_x.0.shortcut.1.weight \t 1024\n",
      "conv5_x.0.shortcut.1.bias \t 1024\n",
      "conv5_x.1.residual_function.0.weight \t 524288\n",
      "conv5_x.1.residual_function.1.weight \t 512\n",
      "conv5_x.1.residual_function.1.bias \t 512\n",
      "conv5_x.1.residual_function.3.weight \t 2359296\n",
      "conv5_x.1.residual_function.4.weight \t 512\n",
      "conv5_x.1.residual_function.4.bias \t 512\n",
      "conv5_x.1.residual_function.6.weight \t 524288\n",
      "conv5_x.1.residual_function.7.weight \t 1024\n",
      "conv5_x.1.residual_function.7.bias \t 1024\n",
      "conv5_x.2.residual_function.0.weight \t 524288\n",
      "conv5_x.2.residual_function.1.weight \t 512\n",
      "conv5_x.2.residual_function.1.bias \t 512\n",
      "conv5_x.2.residual_function.3.weight \t 2359296\n",
      "conv5_x.2.residual_function.4.weight \t 512\n",
      "conv5_x.2.residual_function.4.bias \t 512\n",
      "conv5_x.2.residual_function.6.weight \t 524288\n",
      "conv5_x.2.residual_function.7.weight \t 1024\n",
      "conv5_x.2.residual_function.7.bias \t 1024\n",
      "fc.weight \t 102400\n",
      "fc.bias \t 100\n",
      "\n",
      "Total \t 16833700\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "print('Trainable parameters:')\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, '\\t', param.numel())\n",
    "        total += param.numel()\n",
    "print()\n",
    "print('Total', '\\t', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HC8bfyAWYAN6",
    "outputId": "699aad29-6a48-433a-a1c0-dceca66bc885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 0/196\n",
      "epoch: 0 7/196\n",
      "epoch: 0 14/196\n",
      "epoch: 0 21/196\n",
      "[1,    25] loss: 4.727\n",
      "epoch: 0 28/196\n",
      "epoch: 0 35/196\n",
      "epoch: 0 42/196\n",
      "[1,    50] loss: 4.375\n",
      "epoch: 0 49/196\n",
      "epoch: 0 56/196\n",
      "epoch: 0 63/196\n",
      "epoch: 0 70/196\n",
      "[1,    75] loss: 4.155\n",
      "epoch: 0 77/196\n",
      "epoch: 0 84/196\n",
      "epoch: 0 91/196\n",
      "epoch: 0 98/196\n",
      "[1,   100] loss: 4.035\n",
      "epoch: 0 105/196\n",
      "epoch: 0 112/196\n",
      "epoch: 0 119/196\n",
      "[1,   125] loss: 3.962\n",
      "epoch: 0 126/196\n",
      "epoch: 0 133/196\n",
      "epoch: 0 140/196\n",
      "epoch: 0 147/196\n",
      "[1,   150] loss: 3.881\n",
      "epoch: 0 154/196\n",
      "epoch: 0 161/196\n",
      "epoch: 0 168/196\n",
      "[1,   175] loss: 3.791\n",
      "epoch: 0 175/196\n",
      "epoch: 0 182/196\n",
      "epoch: 0 189/196\n",
      "Accuracy train 12 %\n",
      "Accuracy test 12 %\n",
      "Saving for Accuracy test: 12.53 %\n",
      "epoch: 1 0/196\n",
      "epoch: 1 7/196\n",
      "epoch: 1 14/196\n",
      "epoch: 1 21/196\n",
      "[2,    25] loss: 6.776\n",
      "epoch: 1 28/196\n",
      "epoch: 1 35/196\n",
      "epoch: 1 42/196\n",
      "[2,    50] loss: 3.617\n",
      "epoch: 1 49/196\n",
      "epoch: 1 56/196\n",
      "epoch: 1 63/196\n",
      "epoch: 1 70/196\n",
      "[2,    75] loss: 3.557\n",
      "epoch: 1 77/196\n",
      "epoch: 1 84/196\n",
      "epoch: 1 91/196\n",
      "epoch: 1 98/196\n",
      "[2,   100] loss: 3.499\n",
      "epoch: 1 105/196\n",
      "epoch: 1 112/196\n",
      "epoch: 1 119/196\n",
      "[2,   125] loss: 3.422\n",
      "epoch: 1 126/196\n",
      "epoch: 1 133/196\n",
      "epoch: 1 140/196\n",
      "epoch: 1 147/196\n",
      "[2,   150] loss: 3.388\n",
      "epoch: 1 154/196\n",
      "epoch: 1 161/196\n",
      "epoch: 1 168/196\n",
      "[2,   175] loss: 3.291\n",
      "epoch: 1 175/196\n",
      "epoch: 1 182/196\n",
      "epoch: 1 189/196\n",
      "Accuracy train 20 %\n",
      "Accuracy test 19 %\n",
      "Saving for Accuracy test: 19.73 %\n",
      "epoch: 2 0/196\n",
      "epoch: 2 7/196\n",
      "epoch: 2 14/196\n",
      "epoch: 2 21/196\n",
      "[3,    25] loss: 5.885\n",
      "epoch: 2 28/196\n",
      "epoch: 2 35/196\n",
      "epoch: 2 42/196\n",
      "[3,    50] loss: 3.103\n",
      "epoch: 2 49/196\n",
      "epoch: 2 56/196\n",
      "epoch: 2 63/196\n",
      "epoch: 2 70/196\n",
      "[3,    75] loss: 3.034\n",
      "epoch: 2 77/196\n",
      "epoch: 2 84/196\n",
      "epoch: 2 91/196\n",
      "epoch: 2 98/196\n",
      "[3,   100] loss: 2.985\n",
      "epoch: 2 105/196\n",
      "epoch: 2 112/196\n",
      "epoch: 2 119/196\n",
      "[3,   125] loss: 2.974\n",
      "epoch: 2 126/196\n",
      "epoch: 2 133/196\n",
      "epoch: 2 140/196\n",
      "epoch: 2 147/196\n",
      "[3,   150] loss: 2.848\n",
      "epoch: 2 154/196\n",
      "epoch: 2 161/196\n",
      "epoch: 2 168/196\n",
      "[3,   175] loss: 2.865\n",
      "epoch: 2 175/196\n",
      "epoch: 2 182/196\n",
      "epoch: 2 189/196\n",
      "Accuracy train 28 %\n",
      "Accuracy test 26 %\n",
      "Saving for Accuracy test: 26.44 %\n",
      "epoch: 3 0/196\n",
      "epoch: 3 7/196\n",
      "epoch: 3 14/196\n",
      "epoch: 3 21/196\n",
      "[4,    25] loss: 5.105\n",
      "epoch: 3 28/196\n",
      "epoch: 3 35/196\n",
      "epoch: 3 42/196\n",
      "[4,    50] loss: 2.720\n",
      "epoch: 3 49/196\n",
      "epoch: 3 56/196\n",
      "epoch: 3 63/196\n",
      "epoch: 3 70/196\n",
      "[4,    75] loss: 2.679\n",
      "epoch: 3 77/196\n",
      "epoch: 3 84/196\n",
      "epoch: 3 91/196\n",
      "epoch: 3 98/196\n",
      "[4,   100] loss: 2.614\n",
      "epoch: 3 105/196\n",
      "epoch: 3 112/196\n",
      "epoch: 3 119/196\n",
      "[4,   125] loss: 2.584\n",
      "epoch: 3 126/196\n",
      "epoch: 3 133/196\n",
      "epoch: 3 140/196\n",
      "epoch: 3 147/196\n",
      "[4,   150] loss: 2.493\n",
      "epoch: 3 154/196\n",
      "epoch: 3 161/196\n",
      "epoch: 3 168/196\n",
      "[4,   175] loss: 2.511\n",
      "epoch: 3 175/196\n",
      "epoch: 3 182/196\n",
      "epoch: 3 189/196\n",
      "Accuracy train 36 %\n",
      "Accuracy test 34 %\n",
      "Saving for Accuracy test: 34.73 %\n",
      "epoch: 4 0/196\n",
      "epoch: 4 7/196\n",
      "epoch: 4 14/196\n",
      "epoch: 4 21/196\n",
      "[5,    25] loss: 4.485\n",
      "epoch: 4 28/196\n",
      "epoch: 4 35/196\n",
      "epoch: 4 42/196\n",
      "[5,    50] loss: 2.363\n",
      "epoch: 4 49/196\n",
      "epoch: 4 56/196\n",
      "epoch: 4 63/196\n",
      "epoch: 4 70/196\n",
      "[5,    75] loss: 2.370\n",
      "epoch: 4 77/196\n",
      "epoch: 4 84/196\n",
      "epoch: 4 91/196\n",
      "epoch: 4 98/196\n",
      "[5,   100] loss: 2.284\n",
      "epoch: 4 105/196\n",
      "epoch: 4 112/196\n",
      "epoch: 4 119/196\n",
      "[5,   125] loss: 2.294\n",
      "epoch: 4 126/196\n",
      "epoch: 4 133/196\n",
      "epoch: 4 140/196\n",
      "epoch: 4 147/196\n",
      "[5,   150] loss: 2.281\n",
      "epoch: 4 154/196\n",
      "epoch: 4 161/196\n",
      "epoch: 4 168/196\n",
      "[5,   175] loss: 2.255\n",
      "epoch: 4 175/196\n",
      "epoch: 4 182/196\n",
      "epoch: 4 189/196\n",
      "Accuracy train 41 %\n",
      "Accuracy test 38 %\n",
      "Saving for Accuracy test: 38.83 %\n",
      "epoch: 5 0/196\n",
      "epoch: 5 7/196\n",
      "epoch: 5 14/196\n",
      "epoch: 5 21/196\n",
      "[6,    25] loss: 4.037\n",
      "epoch: 5 28/196\n",
      "epoch: 5 35/196\n",
      "epoch: 5 42/196\n",
      "[6,    50] loss: 2.154\n",
      "epoch: 5 49/196\n",
      "epoch: 5 56/196\n",
      "epoch: 5 63/196\n",
      "epoch: 5 70/196\n",
      "[6,    75] loss: 2.121\n",
      "epoch: 5 77/196\n",
      "epoch: 5 84/196\n",
      "epoch: 5 91/196\n",
      "epoch: 5 98/196\n",
      "[6,   100] loss: 2.135\n",
      "epoch: 5 105/196\n",
      "epoch: 5 112/196\n",
      "epoch: 5 119/196\n",
      "[6,   125] loss: 2.042\n",
      "epoch: 5 126/196\n",
      "epoch: 5 133/196\n",
      "epoch: 5 140/196\n",
      "epoch: 5 147/196\n",
      "[6,   150] loss: 2.045\n",
      "epoch: 5 154/196\n",
      "epoch: 5 161/196\n",
      "epoch: 5 168/196\n",
      "[6,   175] loss: 2.029\n",
      "epoch: 5 175/196\n",
      "epoch: 5 182/196\n",
      "epoch: 5 189/196\n",
      "Accuracy train 46 %\n",
      "Accuracy test 42 %\n",
      "Saving for Accuracy test: 42.16 %\n",
      "epoch: 6 0/196\n",
      "epoch: 6 7/196\n",
      "epoch: 6 14/196\n",
      "epoch: 6 21/196\n",
      "[7,    25] loss: 3.626\n",
      "epoch: 6 28/196\n",
      "epoch: 6 35/196\n",
      "epoch: 6 42/196\n",
      "[7,    50] loss: 1.950\n",
      "epoch: 6 49/196\n",
      "epoch: 6 56/196\n",
      "epoch: 6 63/196\n",
      "epoch: 6 70/196\n",
      "[7,    75] loss: 1.944\n",
      "epoch: 6 77/196\n",
      "epoch: 6 84/196\n",
      "epoch: 6 91/196\n",
      "epoch: 6 98/196\n",
      "[7,   100] loss: 1.894\n",
      "epoch: 6 105/196\n",
      "epoch: 6 112/196\n",
      "epoch: 6 119/196\n",
      "[7,   125] loss: 1.918\n",
      "epoch: 6 126/196\n",
      "epoch: 6 133/196\n",
      "epoch: 6 140/196\n",
      "epoch: 6 147/196\n",
      "[7,   150] loss: 1.885\n",
      "epoch: 6 154/196\n",
      "epoch: 6 161/196\n",
      "epoch: 6 168/196\n",
      "[7,   175] loss: 1.871\n",
      "epoch: 6 175/196\n",
      "epoch: 6 182/196\n",
      "epoch: 6 189/196\n",
      "Accuracy train 49 %\n",
      "Accuracy test 45 %\n",
      "Saving for Accuracy test: 45.55 %\n",
      "epoch: 7 0/196\n",
      "epoch: 7 7/196\n",
      "epoch: 7 14/196\n",
      "epoch: 7 21/196\n",
      "[8,    25] loss: 3.344\n",
      "epoch: 7 28/196\n",
      "epoch: 7 35/196\n",
      "epoch: 7 42/196\n",
      "[8,    50] loss: 1.751\n",
      "epoch: 7 49/196\n",
      "epoch: 7 56/196\n",
      "epoch: 7 63/196\n",
      "epoch: 7 70/196\n",
      "[8,    75] loss: 1.804\n",
      "epoch: 7 77/196\n",
      "epoch: 7 84/196\n",
      "epoch: 7 91/196\n",
      "epoch: 7 98/196\n",
      "[8,   100] loss: 1.777\n",
      "epoch: 7 105/196\n",
      "epoch: 7 112/196\n",
      "epoch: 7 119/196\n",
      "[8,   125] loss: 1.775\n",
      "epoch: 7 126/196\n",
      "epoch: 7 133/196\n",
      "epoch: 7 140/196\n",
      "epoch: 7 147/196\n",
      "[8,   150] loss: 1.763\n",
      "epoch: 7 154/196\n",
      "epoch: 7 161/196\n",
      "epoch: 7 168/196\n",
      "[8,   175] loss: 1.697\n",
      "epoch: 7 175/196\n",
      "epoch: 7 182/196\n",
      "epoch: 7 189/196\n",
      "Accuracy train 54 %\n",
      "Accuracy test 49 %\n",
      "Saving for Accuracy test: 49.56 %\n",
      "epoch: 8 0/196\n",
      "epoch: 8 7/196\n",
      "epoch: 8 14/196\n",
      "epoch: 8 21/196\n",
      "[9,    25] loss: 3.105\n",
      "epoch: 8 28/196\n",
      "epoch: 8 35/196\n",
      "epoch: 8 42/196\n",
      "[9,    50] loss: 1.670\n",
      "epoch: 8 49/196\n",
      "epoch: 8 56/196\n",
      "epoch: 8 63/196\n",
      "epoch: 8 70/196\n",
      "[9,    75] loss: 1.638\n",
      "epoch: 8 77/196\n",
      "epoch: 8 84/196\n",
      "epoch: 8 91/196\n",
      "epoch: 8 98/196\n",
      "[9,   100] loss: 1.631\n",
      "epoch: 8 105/196\n",
      "epoch: 8 112/196\n",
      "epoch: 8 119/196\n",
      "[9,   125] loss: 1.615\n",
      "epoch: 8 126/196\n",
      "epoch: 8 133/196\n",
      "epoch: 8 140/196\n",
      "epoch: 8 147/196\n",
      "[9,   150] loss: 1.689\n",
      "epoch: 8 154/196\n",
      "epoch: 8 161/196\n",
      "epoch: 8 168/196\n",
      "[9,   175] loss: 1.582\n",
      "epoch: 8 175/196\n",
      "epoch: 8 182/196\n",
      "epoch: 8 189/196\n",
      "Accuracy train 56 %\n",
      "Accuracy test 50 %\n",
      "Saving for Accuracy test: 50.50 %\n",
      "epoch: 9 0/196\n",
      "epoch: 9 7/196\n",
      "epoch: 9 14/196\n",
      "epoch: 9 21/196\n",
      "[10,    25] loss: 2.865\n",
      "epoch: 9 28/196\n",
      "epoch: 9 35/196\n",
      "epoch: 9 42/196\n",
      "[10,    50] loss: 1.549\n",
      "epoch: 9 49/196\n",
      "epoch: 9 56/196\n",
      "epoch: 9 63/196\n",
      "epoch: 9 70/196\n",
      "[10,    75] loss: 1.518\n",
      "epoch: 9 77/196\n",
      "epoch: 9 84/196\n",
      "epoch: 9 91/196\n",
      "epoch: 9 98/196\n",
      "[10,   100] loss: 1.552\n",
      "epoch: 9 105/196\n",
      "epoch: 9 112/196\n",
      "epoch: 9 119/196\n",
      "[10,   125] loss: 1.512\n",
      "epoch: 9 126/196\n",
      "epoch: 9 133/196\n",
      "epoch: 9 140/196\n",
      "epoch: 9 147/196\n",
      "[10,   150] loss: 1.500\n",
      "epoch: 9 154/196\n",
      "epoch: 9 161/196\n",
      "epoch: 9 168/196\n",
      "[10,   175] loss: 1.505\n",
      "epoch: 9 175/196\n",
      "epoch: 9 182/196\n",
      "epoch: 9 189/196\n",
      "Accuracy train 58 %\n",
      "Accuracy test 51 %\n",
      "Saving for Accuracy test: 51.50 %\n",
      "epoch: 10 0/196\n",
      "epoch: 10 7/196\n",
      "epoch: 10 14/196\n",
      "epoch: 10 21/196\n",
      "[11,    25] loss: 2.732\n",
      "epoch: 10 28/196\n",
      "epoch: 10 35/196\n",
      "epoch: 10 42/196\n",
      "[11,    50] loss: 1.408\n",
      "epoch: 10 49/196\n",
      "epoch: 10 56/196\n",
      "epoch: 10 63/196\n",
      "epoch: 10 70/196\n",
      "[11,    75] loss: 1.454\n",
      "epoch: 10 77/196\n",
      "epoch: 10 84/196\n",
      "epoch: 10 91/196\n",
      "epoch: 10 98/196\n",
      "[11,   100] loss: 1.427\n",
      "epoch: 10 105/196\n",
      "epoch: 10 112/196\n",
      "epoch: 10 119/196\n",
      "[11,   125] loss: 1.417\n",
      "epoch: 10 126/196\n",
      "epoch: 10 133/196\n",
      "epoch: 10 140/196\n",
      "epoch: 10 147/196\n",
      "[11,   150] loss: 1.442\n",
      "epoch: 10 154/196\n",
      "epoch: 10 161/196\n",
      "epoch: 10 168/196\n",
      "[11,   175] loss: 1.415\n",
      "epoch: 10 175/196\n",
      "epoch: 10 182/196\n",
      "epoch: 10 189/196\n",
      "Accuracy train 60 %\n",
      "Accuracy test 53 %\n",
      "Saving for Accuracy test: 53.41 %\n",
      "epoch: 11 0/196\n",
      "epoch: 11 7/196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 26\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m25\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m24\u001b[39m: \u001b[38;5;66;03m# print every 500 mini batches\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%5d\u001b[39;00m\u001b[38;5;124m] loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,running_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m25\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_gpu = True\n",
    "train_acc = []\n",
    "start = time.time()\n",
    "loss_list = []\n",
    "running_loss = 0\n",
    "total_step = len(trainloader)\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):       \n",
    "        # gpu\n",
    "        if use_gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "            \n",
    "        \n",
    "        # backward and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 25 == 24: # print every 500 mini batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i+1,running_loss/25))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        if i % 7 == 0:\n",
    "            print(\"epoch: {} {}/{}\".format(epoch,i,total_step))\n",
    "\n",
    "\n",
    "    # train\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "\n",
    "            \n",
    "            # gpu\n",
    "            if use_gpu:\n",
    "                if torch.cuda.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(\"Accuracy train %d %%\"%(100*correct/total))\n",
    "    train_acc.append(100*correct/total)\n",
    "\n",
    "    # test\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data      \n",
    "            # gpu\n",
    "            if use_gpu:\n",
    "                if torch.cuda.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)                \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data,1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(\"Accuracy test %d %%\"%(100*correct/total))\n",
    "    accuracy = 100 * correct / total\n",
    "    # Save model weights if the current accuracy is the best so far\n",
    "    if accuracy > best_accuracy:\n",
    "        print(\"Saving for Accuracy test: %.2f %%\" % accuracy)\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), \"best_model_weightsResNet50.pth\")\n",
    "    train_acc.append(100*correct/total)\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "\n",
    "end= time.time()\n",
    "stopWatch = end-start\n",
    "print( \"Training is done\")\n",
    "print('Total Training Time (second):',stopWatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model_weightsResNet50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv2FUk55erp0"
   },
   "outputs": [],
   "source": [
    "classes = ['beaver', 'dolphin', 'otter', 'seal', 'whale', \n",
    "'aquarium' ,'fish', 'ray', 'shark', 'trout', \n",
    "'orchids', 'poppies', 'roses', 'sunflowers', 'tulips', \n",
    "'bottles', 'bowls', 'cans', 'cups', 'plates', \n",
    "'apples', 'mushrooms', 'oranges', 'pears', 'sweet peppers', \n",
    "'clock', 'computer keyboard', 'lamp', 'telephone', 'television', 'bed', 'chair', 'couch', 'table', 'wardrobe', \n",
    "'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach', \n",
    "'bear', 'leopard', 'lion', 'tiger', 'wolf', \n",
    "'bridge', 'castle', 'house', 'road', 'skyscraper', \n",
    "'cloud', 'forest', 'mountain', 'plain', 'sea', \n",
    "'camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo', \n",
    "'fox', 'porcupine', 'possum', 'raccoon', 'skunk', \n",
    "'crab', 'lobster', 'snail', 'spider', 'worm', \n",
    "'baby', 'boy', 'girl', 'man', 'woman', \n",
    "'crocodile', 'dinosaur', 'lizard', 'snake', 'turtle', \n",
    "'hamster', 'mouse', 'rabbit', 'shrew', 'squirrel', \n",
    "'maple', 'oak', 'palm', 'pine', 'willow', \n",
    "'bicycle', 'bus', 'motorcycle', 'pickup truck', 'train', \n",
    "'lawn-mower', 'rocket', 'streetcar', 'tank', 'tractor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oe0fYiBsbrA4",
    "outputId": "357c78c7-a1c5-47ec-e6d0-b5bade237f7c"
   },
   "outputs": [],
   "source": [
    "def test_label_predictions(model, device, test_loader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            actuals.extend(target.view_as(prediction))\n",
    "            predictions.extend(prediction)\n",
    "    return [i.item() for i in actuals], [i.item() for i in predictions]\n",
    "\n",
    "actuals, predictions = test_label_predictions(model, device, testloader)\n",
    "print('F1 score: %f' % f1_score(actuals, predictions, average='weighted'))\n",
    "print('Accuracy score: %f' % accuracy_score(actuals, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "colab_type": "code",
    "id": "zV6jJA9zbrDP",
    "outputId": "af4795b1-9ad5-4638-f464-a2aa7eff3e00"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(actuals, predictions)\n",
    "print(cm)\n",
    "fig = plt.figure(figsize=(24,24))\n",
    "ax = fig.add_subplot(211)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + classes)\n",
    "ax.set_yticklabels([''] + classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "m9cYVsALb6EI",
    "outputId": "c0433066-5e46-4f05-da4e-1ac47129eb16"
   },
   "outputs": [],
   "source": [
    " print(classification_report(actuals, predictions, target_names=classes, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGOiuFRlGjqY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ResNet50 on Cifar 100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "17ab7edf3843410182a0e1b1d2f992fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b89d0abb7194008bbce622ed5687265",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3dd48e39941c4b83a6bb5181fccf9554",
      "value": 1
     }
    },
    "3dd48e39941c4b83a6bb5181fccf9554": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "74ff845e25604f548f47fea8b629a4aa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b89d0abb7194008bbce622ed5687265": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bffefb88a124d4a9c0c209b1bde1950": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17ab7edf3843410182a0e1b1d2f992fe",
       "IPY_MODEL_a923df6caf654c10adc55ff9c519bb2d"
      ],
      "layout": "IPY_MODEL_74ff845e25604f548f47fea8b629a4aa"
     }
    },
    "a15ab6aea3ca43809d12366dd3563b2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a923df6caf654c10adc55ff9c519bb2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc5d9dfdd01546b281c589d9b3e93959",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a15ab6aea3ca43809d12366dd3563b2c",
      "value": " 169009152/? [00:30&lt;00:00, 17839992.67it/s]"
     }
    },
    "fc5d9dfdd01546b281c589d9b3e93959": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
